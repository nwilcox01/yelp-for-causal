pred_a <- predict(est_a, newdata=validation, type="response")
pred_b <- predict(est_b, newdata=validation, type="response")
pred_c <- predict(est_c, newdata=validation, type="response")
pred_d <- predict(est_d, newdata=validation, type="response")
# * MSE
cvrisk[fold,] <- c(est_mse(pred_a, validation$Y),
est_mse(pred_b, validation$Y),
est_mse(pred_c, validation$Y),
est_mse(pred_d, validation$Y))
cvrisk
library(dplyr)
library(ggplot2)
obsdata <- read.csv("RAssign3.csv")
summary(obsdata)
n <- nrow(obsdata)
n
obsdata <- obsdata %>% mutate(Y     = Y < 110,
sinW3 = sin(W3),
W4sq  = W4^2,
cosW5 = cos(W5))
obsdata <- obsdata %>% mutate(Fold=rep(1:20, each=250))
cvrisk <- matrix(nrow=20, ncol=4)
obsdata <- obsdata %>% mutate(Y     = as.numeric(Y < 110),
sinW3 = sin(W3),
W4sq  = W4^2,
cosW5 = cos(W5))
obsdata <- obsdata %>% mutate(Fold=rep(1:20, each=250))
cvrisk <- matrix(nrow=20, ncol=4)
library(dplyr)
library(ggplot2)
obsdata <- read.csv("RAssign3.csv")
obsdata <- obsdata %>% mutate(Y     = as.numeric(Y < 110),
sinW3 = sin(W3),
W4sq  = W4^2,
cosW5 = cos(W5))
obsdata
obsdata <- obsdata %>% mutate(Fold=rep(1:20, each=250))
cvrisk <- matrix(nrow=20, ncol=4)
est_mse <- function(predictions, truth) {
mean((truth-predictions)^2)
}
# fold <- 1
for (fold in 1:20) {
# * TRAINING AND VALIDATION
training   <- obsdata %>% filter(Fold==fold)
validation <- obsdata %>% filter(Fold!=fold)
# * FIT ALGORITHMS
est_a <- glm(Y ~ W1 + W2 + sinW3 + W4sq, family="binomial", data=training)
est_b <- glm(Y ~ W1 + W2 + W4 + cosW5, family="binomial", data=training)
est_c <- glm(Y ~ W2 + W3 + W5 + W2*W5 + W4sq +cosW5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training)
# * GET PREDICTIONS
pred_a <- predict(est_a, newdata=validation, type="response")
pred_b <- predict(est_b, newdata=validation, type="response")
pred_c <- predict(est_c, newdata=validation, type="response")
pred_d <- predict(est_d, newdata=validation, type="response")
# * MSE
cvrisk[fold,] <- c(est_mse(pred_a, validation$Y),
est_mse(pred_b, validation$Y),
est_mse(pred_c, validation$Y),
est_mse(pred_d, validation$Y))
}
fold
# * TRAINING AND VALIDATION
training   <- obsdata %>% filter(Fold==fold)
validation <- obsdata %>% filter(Fold!=fold)
# * FIT ALGORITHMS
est_a <- glm(Y ~ W1 + W2 + sinW3 + W4sq, family="binomial", data=training)
est_b <- glm(Y ~ W1 + W2 + W4 + cosW5, family="binomial", data=training)
est_c <- glm(Y ~ W2 + W3 + W5 + W2*W5 + W4sq +cosW5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training)
# * GET PREDICTIONS
pred_a <- predict(est_a, newdata=validation, type="response")
pred_b <- predict(est_b, newdata=validation, type="response")
pred_c <- predict(est_c, newdata=validation, type="response")
pred_d <- predict(est_d, newdata=validation, type="response")
# * MSE
cvrisk[fold,] <- c(est_mse(pred_a, validation$Y),
est_mse(pred_b, validation$Y),
est_mse(pred_c, validation$Y),
est_mse(pred_d, validation$Y))
cvrisk
# fold <- 1
for (fold in 6:20) {
# * TRAINING AND VALIDATION
training   <- obsdata %>% filter(Fold==fold)
validation <- obsdata %>% filter(Fold!=fold)
# * FIT ALGORITHMS
est_a <- glm(Y ~ W1 + W2 + sinW3 + W4sq, family="binomial", data=training)
est_b <- glm(Y ~ W1 + W2 + W4 + cosW5, family="binomial", data=training)
est_c <- glm(Y ~ W2 + W3 + W5 + W2*W5 + W4sq +cosW5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training)
# * GET PREDICTIONS
pred_a <- predict(est_a, newdata=validation, type="response")
pred_b <- predict(est_b, newdata=validation, type="response")
pred_c <- predict(est_c, newdata=validation, type="response")
pred_d <- predict(est_d, newdata=validation, type="response")
# * MSE
cvrisk[fold,] <- c(est_mse(pred_a, validation$Y),
est_mse(pred_b, validation$Y),
est_mse(pred_c, validation$Y),
est_mse(pred_d, validation$Y))
}
fold
cvrisk
nums <- c(1:4,6:20)
fold <- 5
# * TRAINING AND VALIDATION
training   <- obsdata %>% filter(Fold==fold)
validation <- obsdata %>% filter(Fold!=fold)
# * FIT ALGORITHMS
est_a <- glm(Y ~ W1 + W2 + sinW3 + W4sq, family="binomial", data=training)
est_b <- glm(Y ~ W1 + W2 + W4 + cosW5, family="binomial", data=training)
est_c <- glm(Y ~ W2 + W3 + W5 + W2*W5 + W4sq +cosW5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training)
est_d
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training, maxit=50)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training, maxit=100)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5, W2*W5, W1*W2*W5, family="binomial", data=training, maxit=200)
for (fold in nums) {
# * TRAINING AND VALIDATION
training   <- obsdata %>% filter(Fold==fold)
validation <- obsdata %>% filter(Fold!=fold)
# * FIT ALGORITHMS
est_a <- glm(Y ~ W1 + W2 + sinW3 + W4sq, family="binomial", data=training)
est_b <- glm(Y ~ W1 + W2 + W4 + cosW5, family="binomial", data=training)
est_c <- glm(Y ~ W2 + W3 + W5 + W2*W5 + W4sq +cosW5, family="binomial", data=training)
est_d <- glm(Y ~ W1 + W2 + W5 + W1*W2 + W1*W5 + W2*W5 + W1*W2*W5, family="binomial", data=training)
# * GET PREDICTIONS
pred_a <- predict(est_a, newdata=validation, type="response")
pred_b <- predict(est_b, newdata=validation, type="response")
pred_c <- predict(est_c, newdata=validation, type="response")
pred_d <- predict(est_d, newdata=validation, type="response")
# * MSE
cvrisk[fold,] <- c(est_mse(pred_a, validation$Y),
est_mse(pred_b, validation$Y),
est_mse(pred_c, validation$Y),
est_mse(pred_d, validation$Y))
}
cvrisk
colMeans(cvrisk)
which(min(colMeans(cvrisk)))
which.min(colMeans(cvrisk))
colMeans(cvrisk)
colMeans(cvrisk)
which.min(colMeans(cvrisk))
colMeans(cvrisk)
paste0("Estimator ", which.min(colMeans(cvrisk)), " has the lowest C-V risk.")
mean_risk <- data.frame(colMeans(cvrisk))
mean_risk
t(mean_risk)
mean_risk <- data.frame(as.matrix(colMeans(cvrisk), ncol=4))
mean_risk
mean_risk <- data.frame(as.matrix(colMeans(cvrisk), ncol=4))
as.matrix(colMeans(cvrisk), ncol=4)
matrix(colMeans(cvrisk), ncol=4)
mean_risk <- data.frame(matrix(colMeans(cvrisk), ncol=4))
mean_risk
names(mean_risk) <- c("Estimator 1", "Estimator 2", "Estimator 3", "Estimator 4")
mean_risk
paste0("Estimator ", which.min(colMeans(cvrisk)), " has the lowest C-V risk.")
mean_risk
full_est <- glm(Y ~ W1 + W2 + sinW3 + W4sq, family="binomial", data=obsdata)
summary(full_est)
names(mean_risk) <- paste0("Algorithm ", 1:4)
mean_risk
full_est <- glm(Y ~ W1 + W2 + sinW3 + W4sq, family="binomial", data=obsdata)
summary(full_est)
install.packages("SuperLeaner")
install.packages("SuperLearner")
install.packages("SuperLearner")
??SuperLearner
install.packages("SuperLearner")
install.packages("SuperLearner")
install.packages("SuperLearner")
# install.packages("SuperLearner")
library(SuperLearner)
source("RAssign3.Wrappers.R")
sl_library <- c("SL.glm.EstA", "SL.glm.EstB", "SL.glm.EstC", "SL.glm.EstD", "SL.rpartPrune", "SL.polymars", "SL.mean")
X <- obsdata %>% select(-Y, -fold)
library(dplyr)
library(ggplot2)
X <- obsdata %>% select(-Y, -fold)
X
X <- obsdata %>% select(-Y, -Fold)
X
warnings()
SuperLearner(Y=obsdata$Y, X=X, SL.library=sl_library, family="binomial", cvControl=list(V=10))
install.packages("polspline")
sl_out <- SuperLearner(Y=obsdata$Y, X=X, SL.library=sl_library, family="binomial", cvControl=list(V=10))
sl_out
sl_out <- SuperLearner(Y=obsdata$Y, X=X, SL.library=sl_library, family="binomial", cvControl=list(V=20))
sl_out
X <- obsdata %>% select(-Y)
head(X)
sl_out <- SuperLearner(Y=obsdata$Y, X=X, SL.library=sl_library, family="binomial", cvControl=list(V=20))
sl_out <- SuperLearner(Y=obsdata$Y, X=X, SL.library=sl_library, family="binomial", cvControl=list(V=20))
source("RAssign3.Wrappers.R")
sl_library <- c("SL.glm.EstA", "SL.ridge", "SL.rpartPrune",
"SL.polymars", "SL.mean")
sl_out <- SuperLearner(Y=obsdata$Y, X=X, SL.library=sl_library, family="binomial", cvControl=list(V=20))
sl_out
json <- read.csv("Downloads/yelp-recsys-2013/yelp_training_set/yelp_training_set_business.json")
table <- fromJSON(json)
library(jsonlite)
json <- read.csv("Downloads/yelp-recsys-2013/yelp_training_set/yelp_training_set_business.json")
table <- fromJSON(json)
json <- "Downloads/yelp-recsys-2013/yelp_training_set/yelp_training_set_business.json"
table <- fromJSON(json)
yelp <- fromJSON("Downloads/yelp_dataset/business.json")
library(jsonlite)
yelp <- fromJSON("Downloads/yelp_dataset/business.json")
library(jsonlite)
json <- "Downloads/yelp_dataset/business.json"
yelp <- stream_in(file(json))
yelp
head(yelp)
yelp <- flatten(yelp)
head(yelp)
library(tibble)
yelp <- as_data_frame(yelp)
head(yelp)
yelp %>% mutate(categories = as.character(categories)) %>% select(categories)
library(dplyr)
yelp %>% mutate(categories = as.character(categories)) %>% select(categories)
yelp %>% select(-starts_with("hours"), -starts_with("attribute"))
names(yelp)
yelp %>% mutate(categories = as.character(categories)) %>% select(categories)
yelp
yelp %>% mutate(categories = as.character(categories)) %>% select(categories)
yelp
yelp$categories
nrow(yelp)
names(yelp)
yelp <- yelp %>% select(-starts_with("hours"), -starts_with("attribute"))
dim(yelp)
library(tidyr)
yelp %>% select(-starts_with("hours"), -starts_with("attribute")) %>%
filter(str_detect(categories, "Restaurant")) %>%
unnest(categories) %>%
select(name, categories)
library(stringr)
yelp %>% select(-starts_with("hours"), -starts_with("attribute")) %>%
filter(str_detect(categories, "Restaurant")) %>%
unnest(categories) %>%
select(name, categories)
yelp <- yelp_tbl %>% mutate(categories = as.character(categories)) %>% select(categories)
yelp <- yelp %>% mutate(categories = as.character(categories)) %>% select(categories)
yelp %>% select(-starts_with("hours"), -starts_with("attribute")) %>%
filter(str_detect(categories, "Restaurant")) %>%
unnest(categories) %>%
select(name, categories)
library(jsonlite)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
json <- "Downloads/yelp_dataset/business.json"
yelp <- stream_in(file(json))
yelp <- flatten(yelp)
yelp <- as_data_frame(yelp)
head(yelp)
names(yelp)
nrow(yelp)
yelp <- yelp %>% select(-starts_with("hours"), -starts_with("attribute"))
yelp <- yelp %>% mutate(categories = as.character(categories)) %>% select(categories)
dim(yelp)
yelp %>% select(-starts_with("hours"), -starts_with("attribute")) %>%
filter(str_detect(categories, "Restaurant")) %>%
unnest(categories) %>%
select(name, categories)
names(yelp)
names(yelp)
# * THE FOLLOWING SCRIPT FOLLOWS DIRECTIONS FROM
# * URL: https://blog.exploratory.io/working-with-json-data-in-very-simple-way-ad7ebcc0bb89
library(jsonlite)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
json <- "Downloads/yelp_dataset/business.json"
yelp <- stream_in(file(json))
yelp <- flatten(yelp)
yelp <- as_data_frame(yelp)
head(yelp)
names(yelp)
nrow(yelp)
yelp <- yelp %>% select(-starts_with("hours"), -starts_with("attribute"))
yelp <- yelp %>% mutate(categories = as.character(categories))
dim(yelp)
yelp %>% select(-starts_with("hours"), -starts_with("attribute")) %>%
filter(str_detect(categories, "Restaurant")) %>%
unnest(categories) %>%
select(name, categories)
yelp
head(yelp)
names(yelp)
nrow(yelp)
library(readr)
setwd("~/GitHub/yelp-for-causal/yelp-for-causal/code/espejo")
las_vegas <- read_csv("../../data/las-vegas-yelp.csv")
head(las_vegas)
las_vegas <- read_csv("../../data/las-vegas-yelp.csv")[,-1]
head(las_vegas)
# install.packages("rmutil")
library(rmutil)
set.seed(180)
cr_1_track <- c()
ic_1_track <- c()
cr_2_track <- c()
ic_2_track <- c()
a0 <- 1
a02 <- sqrt(2/pi)  # * EXPECTATION
n <- 50
for (ix in 1:1000) {
setting_1 <- rlaplace(n, m=0, s=a0)
setting_2 <- rnorm(n, mean=0, sd=a02)
a_n1 <- sum(abs(setting_1))/n
a_n2 <- sum(abs(setting_2))/n
# *
cr_variance1 <- sqrt(a_n1/n)
upper_cr1    <- a_n1 + 1.96*cr_variance1
lower_cr1    <- a_n1 - 1.96*cr_variance1
if ((a0 > lower_cr1) & (a0 < upper_cr1)) {
cr_1_track <- c(cr_1_track, ix)
}
ic1 <- abs(setting_1) - a_n1
ic_variance1 <- sqrt(var(ic1)/n)
upper_ic1 <- a_n1 + 1.96*ic_variance1
lower_ic1 <- a_n1 - 1.96*ic_variance1
if ((a0 > lower_ic1) & (a0 < upper_ic1)) {
ic_1_track <- c(ic_1_track, ix)
}
cr_variance2 <- sqrt(a_n2/n)
upper_cr2    <- a_n2 + 1.96*cr_variance2
lower_cr2    <- a_n2 - 1.96*cr_variance2
if ((a02 > lower_cr2) & (a02 < upper_cr2)) {
cr_2_track <- c(cr_2_track, ix)
}
ic2 <- abs(setting_2) - a_n2
ic_variance2 <- sqrt(var(ic2)/n)
upper_ic2 <- a_n2 + 1.96*ic_variance2
lower_ic2 <- a_n2 - 1.96*ic_variance2
if ((a02 > lower_ic2) & (a02 < upper_ic2)) {
ic_2_track <- c(ic_2_track, ix)
}
}
length(cr_1_track)/1000
length(ic_1_track)/1000
length(cr_2_track)/1000
length(ic_2_track)/1000
# install.packages("rmutil")
library(rmutil)
set.seed(180)
cr_1_track <- c()
ic_1_track <- c()
cr_2_track <- c()
ic_2_track <- c()
a0 <- 1
a02 <- sqrt(2/pi)  # * EXPECTATION
n <- 50
for (ix in 1:1000) {
setting_1 <- rlaplace(n, m=0, s=a0)
setting_2 <- rnorm(n, mean=0, sd=1)
a_n1 <- sum(abs(setting_1))/n
a_n2 <- sum(abs(setting_2))/n
# *
cr_variance1 <- sqrt(a_n1/n)
upper_cr1    <- a_n1 + 1.96*cr_variance1
lower_cr1    <- a_n1 - 1.96*cr_variance1
if ((a0 > lower_cr1) & (a0 < upper_cr1)) {
cr_1_track <- c(cr_1_track, ix)
}
ic1 <- abs(setting_1) - a_n1
ic_variance1 <- sqrt(var(ic1)/n)
upper_ic1 <- a_n1 + 1.96*ic_variance1
lower_ic1 <- a_n1 - 1.96*ic_variance1
if ((a0 > lower_ic1) & (a0 < upper_ic1)) {
ic_1_track <- c(ic_1_track, ix)
}
cr_variance2 <- sqrt(a_n2/n)
upper_cr2    <- a_n2 + 1.96*cr_variance2
lower_cr2    <- a_n2 - 1.96*cr_variance2
if ((a02 > lower_cr2) & (a02 < upper_cr2)) {
cr_2_track <- c(cr_2_track, ix)
}
ic2 <- abs(setting_2) - a_n2
ic_variance2 <- sqrt(var(ic2)/n)
upper_ic2 <- a_n2 + 1.96*ic_variance2
lower_ic2 <- a_n2 - 1.96*ic_variance2
if ((a02 > lower_ic2) & (a02 < upper_ic2)) {
ic_2_track <- c(ic_2_track, ix)
}
}
length(cr_1_track)/1000
length(ic_1_track)/1000
length(cr_2_track)/1000
length(ic_2_track)/1000
length(cr_1_track)/1000
length(ic_1_track)/1000
length(cr_2_track)/1000
length(ic_2_track)/1000
length(cr_1_track)/1000
length(ic_1_track)/1000
length(cr_2_track)/1000
length(ic_2_track)/1000
"hi"
length(cr_1_track)/1000
length(ic_1_track)/1000
length(cr_2_track)/1000
length(ic_2_track)/1000
library(jsonlite)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
library(ggplot2)
yelp_2019 <- "../../../data/yelp-2019/business.json"
yelp_2019 <- stream_in(file(yelp_2019))
yelp_2019 <- flatten(yelp_2019)
yelp_2019 <- as_tibble(yelp_2019)
wayback <- "../../../data/yelp-2017/yelp_business.csv"
wayback <- read_csv(wayback)
head(wayback)
intersecting_businesses <- intersect(wayback$business_id, yelp_2019$business_id)
wayback_subset          <- wayback %>% filter(business_id %in% intersecting_businesses)
yelp_2019_subset        <- yelp_2019 %>% filter(business_id %in% intersecting_businesses)
yelp_2019_subberset <- yelp_2019_subset %>%
select(business_id, is_open) %>%
rename(open_2019=is_open)
wayback_subset <- wayback_subset %>% rename(open_2017=is_open)
causal_set     <- merge(wayback_subset, yelp_2019_subberset, by="business_id")
# * ONLY KEEP THOSE THAT WERE OPEN IN 2017
causal_set <- causal_set[which(causal_set$open_2017==1),]
causal_lv <- causal_set %>% filter(city=="Las Vegas") %>% filter(state=="NV")
causal_lv <- causal_lv %>% select(-city, -state)
no_addy   <- which(sapply(causal_lv %>% select(address), nchar)<10)
causal_lv <- causal_lv[-no_addy,]
causal_lv <- causal_lv %>%
filter(grepl("Restaurants|Food", categories)) %>%
filter(!grepl("Grocery", categories))
no_addy   <- which(sapply(causal_lv %>% select(address), nchar)<10)
causal_lv <- causal_lv[-no_addy,]
library(jsonlite)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
library(ggplot2)
library(lubridate)
yelp_2019 <- "../../../data/yelp-2019/business.json"
yelp_2019 <- stream_in(file(yelp_2019))
yelp_2019 <- flatten(yelp_2019)
yelp_2019 <- as_tibble(yelp_2019)
wayback <- "../../../data/yelp-2017/yelp_business.csv"
wayback <- read_csv(wayback)
reviews <- read_csv("../../../data/yelp-2017/yelp_review.csv")
intersecting_businesses <- intersect(wayback$business_id, yelp_2019$business_id)
wayback_subset          <- wayback %>% filter(business_id %in% intersecting_businesses)
yelp_2019_subset        <- yelp_2019 %>% filter(business_id %in% intersecting_businesses)
yelp_2019_subberset <- yelp_2019_subset %>%
select(business_id, is_open) %>%
rename(open_2019=is_open)
wayback_subset <- wayback_subset %>% rename(open_2017=is_open)
causal_set     <- merge(wayback_subset, yelp_2019_subberset, by="business_id")
# * ONLY KEEP THOSE THAT WERE OPEN IN 2017
causal_set <- causal_set[which(causal_set$open_2017==1),]
causal_lv <- causal_set %>% filter(city=="Las Vegas") %>% filter(state=="NV")
causal_lv <- causal_lv %>% select(-city, -state)
no_addy   <- which(sapply(causal_lv %>% select(address), nchar)<10)
causal_lv <- causal_lv[-no_addy,]
causal_lv <- causal_lv %>%
filter(grepl("Restaurants|Food", categories)) %>%
filter(!grepl("Grocery", categories))
causal_lv <- causal_lv %>%
filter(!grepl("Walgreens", name)) %>%
filter(!grepl("Specialty Food", categories)) %>%
filter(!grepl("Convenience Stores", categories)) %>%
filter(!grepl("Gas Stations", categories)) %>%
filter(!grepl("Car Wash", categories)) %>%
filter(!grepl("Wine & Spirits", categories)) %>%
filter(!grepl("Festival", categories)) %>%
filter(!grepl("Water Store", categories))
# * GET ID'S TO MATCH TO REVIEWS
lv_business_id <- causal_lv %>% pull(business_id)
lv_reviews <- reviews %>% filter(business_id %in% lv_business_id)
min_dates <- sapply(1:length(lv_business_id), function(ix) {
id <- lv_business_id[ix]
min_date <- lv_reviews %>% filter(business_id==id) %>%
summarize(min_date=min(as.Date(date))) %>%
mutate(min_date=as.character(min_date)) %>%
pull(min_date)
min_date
})
earliest_review <- data.frame(cbind(lv_business_id, min_dates)) %>%
rename(business_id=lv_business_id, age=min_dates)
causal_lv <- inner_join(causal_lv, earliest_review, by="business_id")
causal_lv <- causal_lv %>%
mutate(age=int_length(interval(start=age, end=today()))/60/60/24)
head(causal_lv %>% select(age))
causal_lv
# create binary variable for type of restaurant (American vs. not)
causal_lv <- causal_lv %>%
mutate(american = ifelse(grepl("American|Burgers|Steak", categories), "1", "0"))
head(causal_lv)
# head(causal_lv)
write.csv(x=causal_lv, file="../../data/las-vegas-yelp.csv")
